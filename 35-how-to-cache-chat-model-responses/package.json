{
  "name": "35-how-to-cache-chat-model-responses",
  "version": "1.0.0",
  "description": "This repository demonstrates how to use the Langchain library to interact with a chat model, including caching response results using Redis.",
  "main": "index.js",
  "keywords": [],
  "author": "",
  "license": "ISC",
  "scripts": {
    "index": "node -r ts-node/register --env-file=.env src/index.ts"
  },
  "dependencies": {
    "@langchain/cohere": "^0.3.1",
    "@langchain/community": "^0.3.11",
    "@langchain/core": "0.3.13",
    "@langchain/langgraph": "^0.2.16",
    "@langchain/ollama": "^0.1.1",
    "@langchain/openai": "^0.3.11",
    "hnswlib-node": "^3.0.0",
    "ioredis": "^5.4.1",
    "langchain": "^0.3.5",
    "zod": "^3.23.8"
  },
  "pnpm": {
    "overrides": {
      "@langchain/core": "0.3.13"
    }
  },
  "devDependencies": {
    "@types/node": "^22.7.7",
    "ts-node": "^10.9.2",
    "typescript": "^5.6.3"
  }
}
