# How to stream chat model responses

This project demonstrates how to stream chat model responses using
LangChain.

- **src/index.ts**: This file contains the main code for streaming chat
  model responses. It creates a new instance of the `ChatOllama` model and
  uses its `stream` method to receive response chunks. The response chunks
  are then printed to the console.
- **package.json**: This file contains metadata about the project,
  including its name, version, dependencies, and scripts.
